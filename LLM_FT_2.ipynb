{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "654f0996-ec81-4366-b378-3e51184f4375",
   "metadata": {},
   "source": [
    "What This Script Does\n",
    "\n",
    "Loads TinyLLaMA (1.1B) in 4-bit → fits your 6GB GPU.\n",
    "\n",
    "Creates a tiny JSONL dataset (3 samples).\n",
    "\n",
    "Formats data in Alpaca-style (Instruction, Input, Response).\n",
    "\n",
    "Applies LoRA tuning (very lightweight).\n",
    "\n",
    "Trains for 1 epoch (finishes in a few minutes).\n",
    "\n",
    "Evaluates loss on same tiny dataset.\n",
    "\n",
    "Runs inference to test tuned behavior.\n",
    "\n",
    "Prints hyperparameter tips + impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ad78c30-a75d-438c-ab8b-efb300e12987",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhishek jaiswal\\inkri\\qualcomm_tasks\\llama_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 1) Imports & Setup\n",
    "# ================================\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer,\n",
    "    TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig, get_peft_model,\n",
    "    prepare_model_for_kbit_training, PeftModel\n",
    ")\n",
    "\n",
    "# Model & paths\n",
    "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "OUTPUT_DIR = \"tiny_inst_lora\"\n",
    "JSONL = \"inst_tiny.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e4f9cec-1183-445b-b20f-b57b0a8613f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 3 examples [00:00, 292.81 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 2) Create Tiny Dataset (reduced size)\n",
    "# ================================\n",
    "if not os.path.exists(JSONL):\n",
    "    with open(JSONL, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write('{\"instruction\":\"Summarize in one sentence.\",\"input\":\"The sun warms the Earth and helps plants grow.\",\"output\":\"The sun provides warmth and energy for plant growth.\"}\\n')\n",
    "        f.write('{\"instruction\":\"Translate to French.\",\"input\":\"Good morning, how are you?\",\"output\":\"Bonjour, comment ça va ?\"}\\n')\n",
    "        f.write('{\"instruction\":\"List two fruits.\",\"input\":\"\",\"output\":\"Apple, Banana\"}\\n')\n",
    "\n",
    "raw_dataset = load_dataset(\"json\", data_files=JSONL, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5071222c-f8f0-4f80-b13b-e0d37d7d8629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "The 8-bit optimizer is not available on your device, only available on CUDA for now.\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 3) Load Model & Tokenizer (4-bit)\n",
    "# ================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f223e3e9-002e-456c-ba0a-e66a90bb3a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 4) Apply LoRA Config\n",
    "# ================================\n",
    "lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # lightweight\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "037b76ac-bf95-40a7-b4a1-3c3c703d4991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 264.89 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 215.38 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 5) Format Data (prompt template)\n",
    "# ================================\n",
    "def format_example(example):\n",
    "    prompt = f\"### Instruction:\\n{example['instruction']}\\n\"\n",
    "    if example[\"input\"]:\n",
    "        prompt += f\"### Input:\\n{example['input']}\\n\"\n",
    "    prompt += \"### Response:\\n\"\n",
    "    full_text = prompt + example[\"output\"]\n",
    "    return {\"text\": full_text}\n",
    "\n",
    "formatted = raw_dataset.map(format_example)\n",
    "\n",
    "def tokenize_function(example, max_length=256):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "tokenized = formatted.map(tokenize_function, remove_columns=formatted.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f00a4b9-1b8a-402f-a635-2fa58e09c4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhishek jaiswal\\inkri\\qualcomm_tasks\\llama_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "C:\\Users\\abhishek jaiswal\\inkri\\qualcomm_tasks\\llama_env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.520800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training finished. Adapter saved to tiny_inst_lora\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 6) Training\n",
    "# ================================\n",
    "collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"no\",\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized,\n",
    "    data_collator=collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save adapter\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(\"✅ Training finished. Adapter saved to\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "692fec7f-db4a-4152-93db-28acd19134a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Eval metrics: {'eval_loss': 2.645254135131836, 'eval_model_preparation_time': 0.0062, 'eval_runtime': 17.2008, 'eval_samples_per_second': 0.174, 'eval_steps_per_second': 0.174}\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 7) Evaluation (loss)\n",
    "# ================================\n",
    "eval_args = TrainingArguments(\n",
    "    output_dir=\"eval_tmp\",\n",
    "    per_device_eval_batch_size=1,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=eval_args,\n",
    "    eval_dataset=tokenized,\n",
    "    data_collator=collator\n",
    ")\n",
    "metrics = trainer.evaluate()\n",
    "print(\"📊 Eval metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "511ce8c0-9811-49fa-8b8b-4a29d6e71e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💡 Model output:\n",
      " ### Instruction:\n",
      "Translate to Spanish.\n",
      "### Input:\n",
      "I love learning with small models.\n",
      "### Response:\n",
      "Me encanta aprender con modelos pequeños.\n",
      "### Notes:\n",
      "- The original text is in English. The translated text is in Spanish.\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 8) Inference with Fine-tuned Model\n",
    "# ================================\n",
    "def make_prompt(instruction, inp=\"\"):\n",
    "    txt = f\"### Instruction:\\n{instruction}\\n\"\n",
    "    if inp:\n",
    "        txt += f\"### Input:\\n{inp}\\n\"\n",
    "    txt += \"### Response:\\n\"\n",
    "    return txt\n",
    "\n",
    "# Reload model + adapter\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True\n",
    ")\n",
    "model = PeftModel.from_pretrained(base, OUTPUT_DIR)\n",
    "model.eval()\n",
    "\n",
    "prompt = make_prompt(\"Translate to Spanish.\", \"I love learning with small models.\")\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "out = model.generate(**inputs, max_new_tokens=60, do_sample=True, temperature=0.7)\n",
    "print(\"💡 Model output:\\n\", tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dbfd7a6-0747-44c7-875e-a2b74afab15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================\n",
      "Instruction: Translate to Spanish.\n",
      "Input: I love learning with small models.\n",
      "💡 Model output:\n",
      " ### Instruction:\n",
      "Translate to Spanish.\n",
      "### Input:\n",
      "I love learning with small models.\n",
      "### Response:\n",
      "Enamorándome aprendiendo con modelos pequeños.\n",
      "=======================================\n",
      "\n",
      "=======================================\n",
      "Instruction: Summarize in one sentence.\n",
      "Input: The sun warms the Earth and helps plants grow.\n",
      "💡 Model output:\n",
      " ### Instruction:\n",
      "Summarize in one sentence.\n",
      "### Input:\n",
      "The sun warms the Earth and helps plants grow.\n",
      "### Response:\n",
      "The sun's energy is used to power the Earth's processes, which helps plants grow.\n",
      "=======================================\n",
      "\n",
      "=======================================\n",
      "Instruction: Translate to French.\n",
      "Input: Good morning, how are you?\n",
      "💡 Model output:\n",
      " ### Instruction:\n",
      "Translate to French.\n",
      "### Input:\n",
      "Good morning, how are you?\n",
      "### Response:\n",
      "Je m'appelle Sophie et je suis désormais en vacances, je vais prendre des vacances trois semaines conséquent.\n",
      "\n",
      "### Output:\n",
      "Bonjour, comment ça est?\n",
      "### French:\n",
      "Je m'appelle Soph\n",
      "=======================================\n",
      "\n",
      "=======================================\n",
      "Instruction: List two fruits.\n",
      "💡 Model output:\n",
      " ### Instruction:\n",
      "List two fruits.\n",
      "### Response:\n",
      "Given example:\n",
      "```javascript\n",
      "const fruits = ['apple', 'banana', 'orange', 'kiwi'];\n",
      "console.log(fruits.includes('kiwi')); // Output: true\n",
      "```\n",
      "\n",
      "Explanation:\n",
      "We define an array of three f\n",
      "=======================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 8.1) Inference with Fine-tuned Model\n",
    "# ================================\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "def make_prompt(instruction, inp=\"\"):\n",
    "    txt = f\"### Instruction:\\n{instruction}\\n\"\n",
    "    if inp:\n",
    "        txt += f\"### Input:\\n{inp}\\n\"\n",
    "    txt += \"### Response:\\n\"\n",
    "    return txt\n",
    "\n",
    "# Reload model + adapter\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True\n",
    ")\n",
    "model = PeftModel.from_pretrained(base, OUTPUT_DIR)\n",
    "model.eval()\n",
    "\n",
    "# List of test instructions\n",
    "test_cases = [\n",
    "    {\n",
    "        \"instruction\": \"Translate to Spanish.\",\n",
    "        \"input\": \"I love learning with small models.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Summarize in one sentence.\",\n",
    "        \"input\": \"The sun warms the Earth and helps plants grow.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Translate to French.\",\n",
    "        \"input\": \"Good morning, how are you?\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"List two fruits.\",\n",
    "        \"input\": \"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run inference for each test case\n",
    "for case in test_cases:\n",
    "    prompt = make_prompt(case[\"instruction\"], case[\"input\"])\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=60,\n",
    "        do_sample=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    print(\"=======================================\")\n",
    "    print(f\"Instruction: {case['instruction']}\")\n",
    "    if case[\"input\"]:\n",
    "        print(f\"Input: {case['input']}\")\n",
    "    print(\"💡 Model output:\\n\", tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "    print(\"=======================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18cb9ced-ad9b-42c3-917d-ab03b43eee22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "👉 Hyperparameter notes:\n",
      "- EPOCHS=1 for demo; increase to 3–5 for better results.\n",
      "- LoRA r=4 (small); increase to 8–16 for stronger adaptation.\n",
      "- MAX_LENGTH=256 to fit RTX 4050; can try 512 with gradient checkpointing.\n",
      "- Dataset only 3 samples for speed; add more for real tuning.\n",
      "\n",
      "\n",
      "✅ Post-Tuning Impact:\n",
      "- Model now respects instruction templates (Instruction, Input, Response).\n",
      "- Even with 3 samples, behavior shifts toward following instructions.\n",
      "- With more data, this method can adapt LLaMA models to domains/tasks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 9) Hyperparameter Tips\n",
    "# ================================\n",
    "print(\"\"\"\n",
    "👉 Hyperparameter notes:\n",
    "- EPOCHS=1 for demo; increase to 3–5 for better results.\n",
    "- LoRA r=4 (small); increase to 8–16 for stronger adaptation.\n",
    "- MAX_LENGTH=256 to fit RTX 4050; can try 512 with gradient checkpointing.\n",
    "- Dataset only 3 samples for speed; add more for real tuning.\n",
    "\"\"\")\n",
    "\n",
    "# ================================\n",
    "# 10) Post-Tuning Impact\n",
    "# ================================\n",
    "print(\"\"\"\n",
    "✅ Post-Tuning Impact:\n",
    "- Model now respects instruction templates (Instruction, Input, Response).\n",
    "- Even with 3 samples, behavior shifts toward following instructions.\n",
    "- With more data, this method can adapt LLaMA models to domains/tasks.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d4513d-3239-47f7-bd4c-8e9fe1e870f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fba5f0-b183-450b-b557-e3514e9701c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thank you"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
